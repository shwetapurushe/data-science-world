{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOAL \n",
    "\n",
    "Predicting adoption churn in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jar dependencies , also be careful of version dependencies\n",
    "\n",
    "* hadoop-aws\n",
    "* aws-java-sdk\n",
    "* aws-java-sdk-core\n",
    "* aws-java-sdk-s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/spurushe/spark/spark-3.0.0-preview2-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as pyf\n",
    "from pyspark.sql.types import *\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"churn_pred\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|Column1|Column2|Column3|\n",
      "+-------+-------+-------+\n",
      "|     10|    100|  Hello|\n",
      "|  10000|1000000| Shweta|\n",
      "+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# raw_df = spark.createDataFrame(\n",
    "#     [\n",
    "#         (10, 100, 'Hello'),\n",
    "#         (10000, 1000000, 'Shweta'),\n",
    "        \n",
    "#     ],\n",
    "#     [\"Column1\", \"Column2\", \"Column3\"] # column names\n",
    "# )\n",
    "\n",
    "# raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.179:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-preview2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>churn_pred</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x114408160>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "DATA IMPORT FROM S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important NOTE  \n",
    "ref: https://www.philipphoffmann.de/post/spark-shell-s3a-support/\n",
    "<br/>\n",
    "\n",
    "* Out of the box Spark distribution does not include support for S3 \n",
    "* You can add it by adding the required jars to spark-3.0.0-preview2-bin-hadoop2.7.jars/\n",
    "\n",
    "<br/>\n",
    "\n",
    "1. Choose hadoop-aws version depending on the version of hadoop files in your spark/jars folder here it was 2.7.4 for Spark 3.0.0 preview release 2.0 \n",
    "2. Run in terminal \n",
    "`cd spark/spark-3.0.0-preview2-bin-hadoop2.7/jars/`\n",
    "`curl -L -O https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.4/hadoop-aws-2.7.4.jar`\n",
    "3. Go to the maven artifact page for hadoop-aws-2.7.4 (https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/2.7.4)\n",
    "4. Look for the Compile dependencies > com.amazonaws Â» aws-java-sdk >> Version. This is the version of aws-java-sdk against which hadoop-aws-2.7.4.jar was built. So this version is very \n",
    "important. If you choose any other aws-java-sdk version, THINGS WILL BREAK. In this case it was aws-java-sdk version 1.7.4. \n",
    "5. Using the aws-java-sdk version go fetch aws-java-sdk   \n",
    "`curl -L -O https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar\n",
    "`\n",
    "6. Similarily also fetch aws-java-sdk-core and aws-java-sdk-s3 with the same versions   \n",
    "`curl -L -O https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-core/1.7.4/aws-java-sdk-core-1.7.4.jar\n",
    " curl -L -O https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-core-s3/1.7.4/aws-java-sdk-core-s3-1.7.4.jar\n",
    "`\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = '*'\n",
    "data_path = '*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"*\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"*\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(f's3a://{bucket_name}/{data_path}/dummy_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|This tweet is abo...|\n",
      "|This tweet is abo...|\n",
      "|This tweet is abo...|\n",
      "|This tweet is abo...|\n",
      "|This tweet is abo...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').load(f's3a://{bucket_name}/{data_path}/*.csv', header= True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
