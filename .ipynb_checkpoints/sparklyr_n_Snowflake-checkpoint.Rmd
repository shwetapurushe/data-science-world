---
title: "Snowflake and sparklyr"
output:
  html_document:
    css: style.css
    df_print: paged
---

<br/> 

Couldn't think of a more Christmassy blog post no? Sparkle your analysis with sparklyr and Snowflake(s)!!  
(Yes I know, bad joke, its the end of the year and I need a break alright!!)  

<br/>

> This blog post is going to be about hooking up your data connections using Snowflake and R, and playing with your larger (wink wink) datasets using the sparkle of sparklyr!!  

<br/>
<p>To get started..  
**Snowflake** is a cloud data warehouse suited for big data storage and analysis. Look [here](https://www.snowflake.com/product/). For this post, Im assuming you have an account with Snowflake.  
**sparklyr** is an interface for R users to leverage the awesomeness of [Apache Spark](https://databricks.com/spark/about). [sparklyr](https://spark.rstudio.com/)  lets you run your analysis and machine learning algorithms on very large datasets, that would ordinarily not fit into your local R instance's memory.</p>  
<p>The steps are relatively straightforward, but require reading quite a lot of documentation and might be a little complicated for someone starting out with either Snowflake or Apache Spark. I didn't find any staightforward answers published online either so this might help.</p>


<br/>
<br/>

### 1. Installations

Technology | What to do
------------- | -------------
Snowflake JDBC driver |  [Click here](https://search.maven.org/classic/#search%7Cgav%7C1%7Cg%3A%22net.snowflake%22%20AND%20a%3A%22snowflake-jdbc%22). 
Snowflake Spark Connector | [Click here](https://search.maven.org/classic/#search%7Cga%7C1%7Ca%3A%22spark-snowflake_2.11%22). 
R / RStudio | [Click here](www.rstudio.com)
sparklyr | Install the 'sparklyr' package in R.
Apache Spark | Run *sparklyr::spark_install( version= "2.3.1" )* in R **once**.

<br/>

### 2. Set up Spark Home Env Variable
This is just setting up the enrionmental variable Spark$Home so that you could run spark commands from your terminal directly.


<p>MacOSX users can edit their .bash_profile file and Windows Users can set thier environmental variables directly through the wizard. 
Your spark home should point to your spark installation, like mine was  
**"somepath/spark/spark-2.3.1-bin-hadoop2.7"**.  
Your Spark$Home variable string will depend on where you have installed it and the downloaded versions.</p> 


<br/>

### 3. Configurations
Now you need to use your Snowflake JDBC driver and the Snowflake Spark Connector to configure your spark-shell (your local Apache Spark connection in this case).  
To do this navigate to your local spark installation bin folder and then run the following command. 

<br/>
It should look something like this.  

 ![](ConfigureSparkConnector.png)

<p>Make sure you get your Apache Spark version, Scala version, Snowflake JDBC driver version and Snowflake Spark Connector version according to your preferences. For more advanced details check [this](https://docs.snowflake.net/manuals/user-guide/spark-connector-install.html).</p>


<br/>

### 4. Lets code and test! 

<p>Ive commented most of the code below because it had sensitive login information.</p>
```{r}
suppressPackageStartupMessages(library(sparklyr))
```









```{r Data connections}
### Setting up config to connect to local mode of Spark

# conf = spark_config()
# conf$`sparklyr.cores.local`= 4
# conf$`sparklyr.shell.driver-memory` = "16G"
# conf$`sparklyr.shell.driver-class-path` = "/Library/Java/Extensions/snowflake-jdbc-3.6.12.jar"
# conf$`spraklyr.memory.fraction` = '0.6';
# 
# 
# sc = spark_connect(master = "local", version = '2.3.1', config = conf)
```







```{r Data Import}
## connection with Snowflake using the Snowflake Driver and Spark connector
## you have to configure ur Spark installation to connect to the Spark connector. TODO document the steps for this



#base_connection_string <- "jdbc:snowflake://your_account_name.snowflakecomputing.com/?account=your_account_name"

# opt = list(
#             url = base_connection_string
#             ,user = "snowflake_username"
#             ,password = "snowflake_password"
#             ,warehouse='snowflake_warehouse_name'
#             ,dbtable = "db.schema.your_table_in_Snowflake"
#             )


## importing the above table from SF into a Spark dataframe 
## input_data is a remote tibble  >>  "tbl_spark" "tbl_sql"   "tbl_lazy"  "tbl"  


 # input_data = sc %>%
 #          spark_read_jdbc(sc = ., name = "spark_df_name", 
 #                          options = opt 
 #                          , repartition = 0
 #                          , memory = FALSE
 #                          , overwrite = TRUE
 #                          )


 # input_data %>% tally()
```

<br/>

> This should create the *spark_df_name* spark data frame on your local Spark, which can then be used for analysis, machine learning and much more.

The world is your oyster !! `r emo::ji("alien")`

